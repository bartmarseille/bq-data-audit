{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "BQ-table-data-audit_v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBee23kp3ZPu",
        "colab_type": "text"
      },
      "source": [
        "# BigQuery Table Data Audit\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-YPx0Y5BXBj",
        "colab_type": "text"
      },
      "source": [
        "### Before you begin\n",
        "\n",
        "\n",
        "1.   Use the [Cloud Resource Manager](https://console.cloud.google.com/cloud-resource-manager) to check if you have access to the Cloud Platform project that has access to the table you want to audit.\n",
        "2.   Make sure you have query access to the table you want to audit [Cloud Console](https://console.cloud.google.com).\n",
        "3.   [Enable BigQuery](https://console.cloud.google.com/flows/enableapi?apiid=bigquery) APIs for the project.\n",
        "4. Provide the variables to apply:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcAIDKrKpsIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BigQuery table to audit:\n",
        "project_id = '<your_project_id>'\n",
        "dataset_id = '<your_dataset_id>'\n",
        "table_id = '<your_table_id>'\n",
        "\n",
        "# Use exact count (uses more computational resources)\n",
        "exact_count = True\n",
        "\n",
        "# Calculate field top-n (bigger n uses more computational resources)\n",
        "n=3\n",
        "\n",
        "table_ref = '{}.{}.{}'.format(project_id, dataset_id, table_id)\n",
        "print('Table reference to audit: \\'{}.{}.{}\\''.format(project_id, dataset_id, table_id))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FnJHnyXDEyQ",
        "colab_type": "text"
      },
      "source": [
        "### Provide your credentials to the runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vyFGlOUC_k6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZr4Zq36I6U-",
        "colab_type": "text"
      },
      "source": [
        "### Retrieve BQ table schema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7h3neYZnoX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "def get_table_schema(project_id, dataset_id, table_id):\n",
        "  print('Retrieving table schema for BQ table: \\'{}.{}.{}\\''.format(project_id, dataset_id, table_id))\n",
        "  table_schema = client.query('''\n",
        "    SELECT column_name, data_type\n",
        "    FROM `{}`.{}.INFORMATION_SCHEMA.COLUMNS\n",
        "    WHERE table_name=\"{}\"'''.format(project_id, dataset_id, table_id)).to_dataframe()\n",
        "  return table_schema\n",
        "\n",
        "def parse_field(field_name, data_type, nesting=[]):\n",
        "  if data_type.startswith('ARRAY<') or data_type.startswith('STRUCT<'):\n",
        "    if not nesting:\n",
        "      nesting = [field_name]\n",
        "    else:\n",
        "      nesting[-1] += '.' + field_name\n",
        "    if data_type.startswith('ARRAY<STRUCT<'):\n",
        "      nesting.append(field_name)\n",
        "      return parse_struct(data_type[13:-2], nesting)\n",
        "    elif data_type.startswith('STRUCT<'):    \n",
        "      return parse_struct(data_type[7:-1], nesting)\n",
        "    else: # data_type.startswith('ARRAY<'):\n",
        "      return [{'field_name': field_name, 'data_type': data_type[6:-1], 'nesting': nesting}]\n",
        "  else: # primitive_type\n",
        "    if nesting:\n",
        "      field_name = nesting.pop() + '.' + field_name\n",
        "    return [{'field_name': field_name, 'data_type': data_type, 'nesting': nesting}]\n",
        "      \n",
        "\n",
        "def parse_struct(struct, nesting):\n",
        "  if struct.strip():\n",
        "    field_name = struct.strip().split(' ')[0]\n",
        "    struct = struct.strip()[len(field_name)+1:]\n",
        "    data_type = ''\n",
        "    depth = 0\n",
        "    for char in iter(struct):\n",
        "      if char == '<': \n",
        "        depth += 1\n",
        "      elif char == '>': \n",
        "        depth -= 1\n",
        "      if depth > 0: \n",
        "        pass\n",
        "      elif char == ',': \n",
        "        break\n",
        "      data_type += char\n",
        "    remaining_struct = struct[len(data_type)+2:]\n",
        "    return parse_field(field_name, data_type, nesting.copy()) + parse_struct(remaining_struct, nesting)\n",
        "  else:\n",
        "    return []\n",
        "\n",
        "def get_unnest_statement(nesting):\n",
        "  result = []\n",
        "  if nesting:\n",
        "    for nest in nesting:\n",
        "      result.append(', UNNEST({}) {}'.format(nest, nest.split('.')[-1]))\n",
        "  return ''.join(result)\n",
        "\n",
        "def count_rows(table_ref, nesting):\n",
        "  query = 'SELECT count(*) FROM `{}`'.format(table_ref) + get_unnest_statement(nesting)\n",
        "  query_job = client.query(query)\n",
        "  return list(query_job.result())[0].values()[0]\n",
        "\n",
        "\n",
        "table_schema = get_table_schema(project_id, dataset_id, table_id)\n",
        "\n",
        "fields = []\n",
        "for i, col in table_schema.iterrows(): \n",
        "  print('- checking column: \\'{}\\' [{}/{}]'.format(col['column_name'], i+1, len(table_schema)))\n",
        "  fields += parse_field(col['column_name'], col['data_type'])\n",
        "\n",
        "row_counts = {}\n",
        "print('\\nCounting rows per nesting:')\n",
        "for field in fields:\n",
        "  key = get_unnest_statement(field['nesting'])\n",
        "  if key not in row_counts:\n",
        "    nest = [] if not field['nesting'] else field['nesting'][-1]\n",
        "    print('- counting rows for nesting: \\'{}\\' '.format(nest), end = '')\n",
        "    count = count_rows(table_ref, field['nesting'])\n",
        "    row_counts[key] = count\n",
        "    print(count)\n",
        "  else:\n",
        "    count = row_counts[key]\n",
        "  field['row_count'] = count\n",
        "  field['table_ref'] = table_ref\n",
        "\n",
        "df = pd.DataFrame(fields)\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "311TTWSFJogn",
        "colab_type": "text"
      },
      "source": [
        "### Perform data audit on table schema fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZUREdlru7HX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cardinality(field):\n",
        "  return get_approx_cardinality(field) if not exact_count and field['data_type'] in ('INT64', 'NUMERIC', 'STRING', 'BYTES') else get_exact_cardinality(field)\n",
        "\n",
        "def get_approx_cardinality(field):\n",
        "  print('.', end = '')\n",
        "  query = 'SELECT HLL_COUNT.MERGE(hll_count) approx FROM ( SELECT HLL_COUNT.INIT({}) hll_count FROM `{}`{})'.format(field['field_name'], field['table_ref'], get_unnest_statement(field['nesting']))\n",
        "  query_job = client.query(query)\n",
        "  return list(query_job.result())[0].values()[0]\n",
        "\n",
        "def get_exact_cardinality(field):\n",
        "  print('.', end = '')\n",
        "  query = 'SELECT COUNT(DISTINCT({})) exact FROM `{}`{}'''.format(field['field_name'], field['table_ref'], get_unnest_statement(field['nesting']))\n",
        "  query_job = client.query(query)\n",
        "  return list(query_job.result())[0].values()[0]\n",
        "\n",
        "def count_missing_values(field):\n",
        "  print('.', end = '')\n",
        "  where_clause = '{} IS NULL'.format(field['field_name']) + (' OR TRIM({}) = \"\"'.format(field['field_name']) if field['data_type'] in ('STRING') else '')\n",
        "  query = 'SELECT SUM(1) num_missing FROM `{}`{} WHERE {}'.format(field['table_ref'], get_unnest_statement(field['nesting']), where_clause)\n",
        "  query_job = client.query(query)\n",
        "  result = list(query_job.result())[0].values()[0]\n",
        "  return result if result else 0\n",
        "\n",
        "def get_statistics(field):\n",
        "  print('.', end = '')\n",
        "  field_name = field['field_name']\n",
        "  avg_expr = 'AVG({})'.format(field_name) if field['data_type'] in ('INT64', 'NUMERIC', 'FLOAT64') else 'CAST(\\'NaN\\' AS FLOAT64)'\n",
        "  std_expr = 'STDDEV({})'.format(field_name) if field['data_type'] in ('FLOAT64') else 'CAST(\\'NaN\\' AS FLOAT64)'\n",
        "  query = 'SELECT MIN({}) min, MAX({}) max, {} avg, {} std FROM `{}`{}'''.format(field_name, field_name, avg_expr, std_expr, field['table_ref'], get_unnest_statement(field['nesting']))\n",
        "  query_job = client.query(query)\n",
        "  return list(query_job.result())[0].values()\n",
        "\n",
        "def get_top_n(field, n):\n",
        "  print('.', end = '')\n",
        "  where_clause = '{} IS NOT NULL'.format(field['field_name']) + (' AND TRIM({}) != \"\"'.format(field['field_name']) if field['data_type'] in ('STRING') else '')\n",
        "  query = '''\n",
        "    SELECT ARRAY_AGG(CONCAT(\"'\", CAST(val AS STRING), \"' [\", CAST(cnt AS STRING), \"]\") ORDER BY cnt DESC LIMIT {}) AS top_{}\n",
        "    FROM (SELECT {} AS val, count(*) AS cnt FROM `{}`{}\n",
        "    WHERE {} GROUP BY 1)'''.format(n, n, field['field_name'], field['table_ref'], get_unnest_statement(field['nesting']), where_clause)\n",
        "  query_job = client.query(query)\n",
        "  return ', '.join(list(query_job.result())[0].values()[0])\n",
        "\n",
        "\n",
        "print('Performing data audit on BQ table: \\'{}\\''.format(table_ref))\n",
        "for i, field in enumerate(fields):\n",
        "  print('- auditing field: \\'{}\\' [{}/{}] '.format(field['field_name'], i+1, len(fields)), end = '')\n",
        "  field['cardinality'] = get_cardinality(field)\n",
        "  field['num_missing_values'] = count_missing_values(field)\n",
        "  min_, max_, avg, stddev = get_statistics(field)\n",
        "  field['min'] = min_\n",
        "  field['max'] = max_\n",
        "  field['avg'] = avg\n",
        "  field['stddev'] = stddev\n",
        "  field['top-{}'.format(n)] = get_top_n(field, n)\n",
        "  print('')\n",
        " \n",
        "print('\\ndone\\n')\n",
        "df = pd.DataFrame(fields)\n",
        "del df['nesting']\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbkCSUb850zS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKCmMIaZi9kO",
        "colab_type": "text"
      },
      "source": [
        "### Prepare final data audit DataFrame\n",
        "Here you can add some more derived metrics, like precentages etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyFLM8tjRnie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_audit_df = pd.DataFrame(fields)\n",
        "# remove unneeded dataframe columns\n",
        "del data_audit_df['nesting']\n",
        "# add derived metrics\n",
        "data_audit_df['perc_missing'] = 100*data_audit_df['num_missing_values']/data_audit_df['row_count']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDIsqi_Ujyx2",
        "colab_type": "text"
      },
      "source": [
        "### Export to Google Spreadseet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDSvjivYSBQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from oauth2client.client import GoogleCredentials\n",
        "import gspread\n",
        "import gspread_dataframe as gd\n",
        "import datetime\n",
        "\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "now = datetime.datetime.now()\n",
        "date = now.strftime(\"%Y%m%d\")\n",
        "spreadhseet_name = 'Data_Audit_{}_{}'.format(table_id, date)\n",
        "\n",
        "# create new spreadsheet\n",
        "spreadhseet = gc.create(spreadhseet_name)\n",
        "\n",
        "# open sheet1 of new spreadsheet and add data audit data\n",
        "worksheet = gc.open(spreadhseet_name).sheet1\n",
        "gd.set_with_dataframe(worksheet, data_audit_df)\n",
        "\n",
        "print('Created \\'{}\\' in your Google Drive home folder.\\n\\n{}'.format(spreadhseet_name, spreadhseet.fetch_sheet_metadata()['spreadsheetUrl']))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}